---
title: "ANALYTICAL AVENGERS"
subtitle: "INFO 523 - Project Final"
author: 
  - name: "ANALYTICAL AVENGERS-<br>
    MELIKA AKBARSHARIFI, Divya liladhar Dhole, Mohammad Ali Farmani,<br> H M Abdul Fattah, Gabriel Gedaliah Geffen, Tanya George, Sunday Usman "
    affiliations:
      - name: "School of Information, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

Add project abstract here.

```{python}
#| label: load-pkgs
#| echo: false
#| message: false

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import numpy as np
from scipy.stats import chi2_contingency
import warnings
# Ignore all warnings
warnings.filterwarnings("ignore")
```

```{python}
# Read in the data
url = 'data/crash_data.csv'
crash_data = pd.read_csv(url)
```

```{python}
# Get the distinct data types of features in the DataFrame
distinct_data_types = crash_data.dtypes.unique()

print("Distinct data types of features in crash_data:")
print(distinct_data_types)

# Display the first few rows to understand the structure of the dataset
print('First few rows of the crash data: \n', crash_data.head())
```

### Question 1

```{python}
#| label: summary statistics for numerical variables


print(crash_data.describe())


# Check for missing values in key columns
print(crash_data[['Age', 'Light Conditions', 'Weather Conditions', 'Road Surface Condition']].isnull().sum())




```

```{python}
#| label: Handling-missing-and-duplicate-rows


# Impute missing values for 'Age of Driver using median
crash_data['Age'].fillna(crash_data['Age'].median(), inplace=True)


# Since the missing values for Light, Weather, and Road Conditions are minimal, we'll impute these with the mode
common_light = crash_data['Light Conditions'].mode()[0]
common_weather = crash_data['Weather Conditions'].mode()[0]
common_road_surface = crash_data['Road Surface Condition'].mode()[0]


crash_data['Light Conditions'].fillna(common_light, inplace=True)
crash_data['Weather Conditions'].fillna(common_weather, inplace=True)
crash_data['Road Surface Condition'].fillna(common_road_surface, inplace=True)


# Confirm changes by checking missing values again
print(crash_data[['Age', 'Light Conditions', 'Weather Conditions', 'Road Surface Condition']].isnull().sum())


```

```{python}
#| label: Define age groups for easier analysis


bins = [0, 18, 25, 40, 55, 70, 100]
labels = ['0-18', '19-25', '26-40', '41-55', '56-70', '71+']
crash_data['Age Group'] = pd.cut(crash_data['Age'], bins=bins, labels=labels, right=False)


```

```{python}
#| label: Visualization of age group and crash severity


plt.figure(figsize=(12, 6))
sns.countplot(x='Age Group', hue='Crash Severity', data=crash_data, palette='coolwarm')
plt.title('Crash Severity Distribution by Driver Age Group')
plt.xlabel('Age Group  Driver')
plt.ylabel('Number of Crashes')
plt.legend(title='Severity')
plt.show()


```

```{python}
#| label: Visualizations for crash severity and Light Conditions


plt.figure(figsize=(14, 7))
sns.countplot(x='Light Conditions', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Light Conditions')
plt.xlabel('Light Conditions')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=45)
plt.show()


```

```{python}

# Heatmap of lighting affecting the severity of danger in different age groups

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 25:
        return '20-25'
    elif 25 <= age <= 35:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Light Conditions', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)
plt.figure(figsize=(10, 6))

sns.heatmap(norm_pivot, annot=True, fmt=".2f", linewidths=.5, cmap='plasma')
plt.title('Heatmap of how lighting plays a factor for certain age groups being in more danger:')
plt.xlabel('Light Conditions')
plt.ylabel('Age Group')
plt.show()


```

```{python}
#| label: Visualizations for crash severity and weather Conditions


plt.figure(figsize=(14, 7))
sns.countplot(x='Weather Conditions', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Weather Conditions')
plt.xlabel('Weather Conditions')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=45) 
plt.show()


```

```{python}
# heatmap for weather conditions affecting severity of crashes for certain age groups

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 45:
        return '20-25'
    elif 45 <= age <= 60:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

# Categorizing weather conditions
def weathergroups(weather):
    if weather in ['Blowing sand, snow', 'Blowing sand, snow/Snow', 'Clear/Snow', 'Cloudy/Blowing sand, snow', 'Other/Snow']:
        return 'Snowy conditions'
    elif weather in ['Clear/Rain', 'Fog, smog, smoke/Rain', 'Rain/Blowing sand, snow', 'Rain/Fog, smog, smoke', 'Rain/Severe crosswinds', 'Rain/Unknown']:
        return 'Rainy/foggy conditions'
    elif weather in ['Snow/Clear', 'Snow/Rain', 'Snow/Snow']:
        return 'Mostly Snowy'
    elif weather in ['Clear/Clear', 'Unknown/Clear']:
        return 'Clear weather'
    elif weather in ['Cloudy/Blowing sand, snow', 'Cloudy/Fog, smog, smoke', 'Cloudy/Severe crosswinds', 'Cloudy/Unknown']:
        return 'Cloudy/smog conditions'
    elif weather in ['Sleet, hail(freezing rain or drizzle)', 'Sleet, hail(freezing rain or drizzle)/Fog, smog, smoke', 'Sleet, hail(freezing rain or drizzle)/Severe crosswinds', 'Sleet, hail(freezing rain or drizzle)/Unknown']:
        return 'Hail/Freezing Drizzle'


crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

crash_data['Weather Group'] = crash_data['Weather Conditions'].apply(weathergroups)


# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Weather Group', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)

plt.figure(figsize=(10, 6))
sns.heatmap(norm_pivot, cmap='plasma', annot=True, fmt=".2f", linewidths=.5)
plt.title('Heatmap of how weather plays a factor for certain age groups being in more danger:')
plt.xlabel('Weather Conditions')
plt.ylabel('Age Group')
plt.show()

```

```{python}
#| label: Visualizations for crash severity and road surface Conditions


plt.figure(figsize=(7, 4))
sns.countplot(x='Road Surface Condition', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Road Surface Condition')
plt.xlabel('Road Surface Condition')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=60) 
plt.show()


```

```{python}

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 25:
        return '20-25'
    elif 25 <= age <= 35:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Road Surface Condition', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)
plt.figure(figsize=(10, 6))

sns.heatmap(norm_pivot, annot=True, fmt=".2f", linewidths=.5, cmap='plasma')
plt.title('Heatmap of how Road surfaces play a factor for certain age groups being in more danger:')
plt.xlabel('Road Surface Condition')
plt.ylabel('Age Group')
plt.show()

```

```{python}
#| label: Visualizations for number of crashes by Age Group and Light Conditions


plt.figure(figsize=(9, 6))
sns.countplot(x='Age Group', hue='Light Conditions', data=crash_data, palette='coolwarm')
plt.title('Impact of Light Conditions on number of crashes by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Number of Crashes')
plt.legend(title='Light Conditions')
plt.show()


```

```{python}
#| label: testing association between Light Conditions and Crash Severity within an Age Group


#contingency_table = pd.crosstab(index=crash_data[crash_data['Age Group'] #== '19-25']['Crash Severity'],
                               #columns=crash_data[crash_data['Age Group'] #== '19-25']['Light Conditions'])
#chi2, p_value, dof, expected = chi2_contingency(contingency_table)
#print(f"Chi-square test p-value for 19-25 age group and light conditions: #{p_value}")


```

### Question 2:

```{python}
# Filter rows where the severity is unknow
crash_data = crash_data[crash_data['Crash Severity'] != "Unknown"]

# Add a new column named 'feature_variable'
crash_data['feature_variable'] = [0 if x == 'Property damage only (none injured)' else 1 for x in crash_data['Crash Severity']]

# Drop the 'Crash Severity' column
crash_data = crash_data.drop('Crash Severity', axis=1)
```

```{python}
# Indetify & Visualize relationships between features and the target variable 'feature_variable'

# Instead of using all the features, I am using custom_feature_set 
# for simplicity with 3 most correlated features and 2 least correlated features

numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64'])

correlation_matrix = numerical_cols.corr()

# The correlation of each feature with the target variable 'mc_preschool'
correlation_with_target = correlation_matrix['feature_variable'].abs().sort_values(ascending=False)

most_correlated_features = correlation_with_target[1:4]  # Exclude the target variable itself
least_correlated_features = correlation_with_target[-2:] 

# Convert the indices of selected features to a list and merge them together
most_correlated_features = most_correlated_features.index.tolist()
least_correlated_features = least_correlated_features.index.tolist()
custom_feature_set = most_correlated_features + least_correlated_features

# Visualize relationships between the custom feature set and the target variable using a pairplot
sns.pairplot(numerical_cols, x_vars=custom_feature_set, y_vars='feature_variable', kind='scatter')
plt.show()

```

```{python}
# Data Cleaning
# Analysis of Missing Values

# Find numerical columns
numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64'])

# Calculate missing values count for each numerical column
missing_values_count = numerical_cols.isnull().sum()

# Calculate missing rate for each numerical column
missing_rate = (missing_values_count / len(crash_data)) * 100

missing_data = pd.DataFrame({
    'Missing Values': missing_values_count,
    'Percentage (%)': missing_rate
})

print('Analysis of Missing Values for numerical features: \n\n', missing_data, '\n\n')

# Drop categorical columns with missing rate over 50%
columns_to_drop = missing_rate[missing_rate > 50].index
crash_data = crash_data.drop(columns_to_drop, axis=1)

# Find categorical columns
categorical_columns = crash_data.select_dtypes(include = ['object', 'category'])

# Calculate missing values count for each categorical column
missing_values_count = categorical_columns.isnull().sum()

# Calculate missing rate for each categorical column
missing_rate = (missing_values_count / len(crash_data)) * 100

missing_data = pd.DataFrame({
    'Missing Values': missing_values_count,
    'Percentage (%)': missing_rate
})

print('Analysis of Missing Values for categorical features: \n\n', missing_data, '\n\n')

# Drop categorical columns with missing rate over 50%
columns_to_drop = missing_rate[missing_rate > 50].index
crash_data = crash_data.drop(columns_to_drop, axis=1)
```

```{python}
all_columns = crash_data.select_dtypes(include = ['int64', 'float64', 'object', 'category'])

# Calculate mode for each categorical column
mode_values = all_columns.mode().iloc[0]

# Fill missing values with mode for each column
crash_data = crash_data.fillna(mode_values)

print(crash_data.head())
```

```{python}
# Standardize features with StandardScaler

numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64']).columns

# Exclude our output feature: 'feature_variable'
feature_to_exclude = 'feature_variable'
numerical_cols = numerical_cols.drop(feature_to_exclude, errors='ignore')

scaler = StandardScaler()

# Fit and transform the numerical columns using StandardScaler
crash_data[numerical_cols] = scaler.fit_transform(crash_data[numerical_cols])

# Display the first few rows of the scaled dataset
print(crash_data.head())
```

```{python}
# Encode categorical variables
categorical_columns = crash_data.select_dtypes(include = ['object', 'category']).columns.tolist()

# Convert numerical columns to strings if necessary
crash_data[categorical_columns] = crash_data[categorical_columns].astype(str)

# Encode categorical variables
label_encoders = {col: LabelEncoder() for col in categorical_columns}
for col in categorical_columns:
    crash_data[col] = label_encoders[col].fit_transform(crash_data[col])

# Convert categorical columns to numerical using one-hot encoding
crash_data = pd.get_dummies(crash_data, columns=categorical_columns)

print(crash_data.head())
```

```{python}
# Define features and target
X = crash_data.drop('feature_variable', axis = 1)
y = crash_data['feature_variable']
```

```{python}
batch_size = 100

# Initialize an empty array to store the selected feature indices
selected_feature_indices = []

# Iterate over the dataset in batches and perform feature selection
for batch_start in range(0, len(X), batch_size):
    X_batch = X.iloc[batch_start:batch_start+batch_size]
    y_batch = y.iloc[batch_start:batch_start+batch_size]
    
    # Perform feature selection on the current batch
    k_best_selector = SelectKBest(score_func=f_classif, k=30)
    X_selected_batch = k_best_selector.fit_transform(X_batch, y_batch)
    
    # Get the indices of the selected features and append to the list
    selected_feature_indices.extend(k_best_selector.get_support(indices=True))

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

print('Selected features: \n\n', selected_features)

# Extract the selected features from the original DataFrame X
X_selected = X[selected_features]
```

```{python}
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Display the shapes of the training and testing sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
```

```{python}
log_reg = LogisticRegression(solver = 'liblinear', max_iter = 1000, random_state = 42)
log_reg.fit(X_train, y_train)

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)

rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_train, y_train)

knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)
```

```{python}
# Model Validation

# List of classifiers
classifiers = [log_reg, dtree, rf_classifier, knn]
# classifiers = [log_reg]

# Perform cross-validation and compute evaluation metrics for each classifier
for classifier in classifiers:
    # Cross-validation
    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)

    # Compute evaluation metrics
    accuracy = cv_scores.mean()
    precision = precision_score(y_test, classifier.predict(X_test))
    recall = recall_score(y_test, classifier.predict(X_test))
    f1 = f1_score(y_test, classifier.predict(X_test))
    
    # Print the results
    print('Classifier: ', str(classifier))
    print('Accuracy: ', accuracy)
    print('Precision: ', precision)
    print('Recall: ', recall)
    print('F1-Score: ', f1)
```

```{python}
probs_knn = knn.predict_proba(X_test)
# Keep probabilities for the positive outcome only
probs_knn = probs_knn[:, 1]
# Compute the ROC curve
fpr_knn, tpr_knn, thresholds = roc_curve(y_test, probs_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

probs_forest = rf_classifier.predict_proba(X_test)
probs_forest = probs_forest[:, 1]
fpr_forest, tpr_forest, thresholds = roc_curve(y_test, probs_forest)
roc_auc_forest = auc(fpr_forest, tpr_forest)


probs_tree = dtree.predict_proba(X_test)
probs_tree = probs_tree[:, 1]
fpr_tree, tpr_tree, thresholds = roc_curve(y_test, probs_tree)
roc_auc_tree = auc(fpr_tree, tpr_tree)


probs_log = log_reg.predict_proba(X_test)
probs_log = probs_log[:, 1]
fpr_log, tpr_log, thresholds = roc_curve(y_test, probs_log)
roc_auc_log = auc(fpr_log, tpr_log)
```

```{python}
plt.figure(figsize=(8, 6))
plt.plot(fpr_knn, tpr_knn, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_knn:.2f}) for KNN')
plt.plot(fpr_tree, tpr_tree, color='blue', lw=2, label=f'ROC curve (area = {roc_auc_tree:.2f}) for Decision Tree')
plt.plot(fpr_forest, tpr_forest, color='red', lw=2, label=f'ROC curve (area = {roc_auc_forest:.2f}) for Random Forest')
plt.plot(fpr_log, tpr_log, color='Green', lw=2, label=f'ROC curve (area = {roc_auc_log:.2f}) for Logistic Regression')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal 45 degree line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

```{python}
### Accuracy on Test data

predictions_log = log_reg.predict(X_test)
print("Logistic Regression Accuracy:", np.round(accuracy_score(y_test, predictions_log),3))

predictions_tree = dtree.predict(X_test)
print("Decision Tree Accuracy:", np.round(accuracy_score(y_test, predictions_tree),3))

predictions_forest = rf_classifier.predict(X_test)
print("Random Forest Accuracy:", np.round(accuracy_score(y_test, predictions_forest),3))


predictions_knn = knn.predict(X_test)
print("KNN Accuracy:",np.round(accuracy_score(y_test, predictions_knn),3))


```

```{python}
## Confusion Matrix
cm = confusion_matrix(y_test, predictions_log)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Logistic Regression Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()
```

```{python}
## Confusion Matrix
cm = confusion_matrix(y_test, predictions_knn)
sns.heatmap(cm, annot = True, fmt ='g')
plt.title('KNN Confusion Matrix') 
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

```

```{python}
cm = confusion_matrix(y_test, predictions_tree)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Decision Tree Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

```

```{python}

cm = confusion_matrix(y_test, predictions_forest)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Random Forest Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()
```
