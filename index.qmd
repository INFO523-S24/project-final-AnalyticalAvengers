---
title: "ANALYTICAL AVENGERS"
subtitle: "INFO 523 - Project Final"
author: 
  - name: "ANALYTICAL AVENGER:-<br>
    Melika Akbarsharifi, Divya liladhar Dhole, Mohammad Ali Farmani,<br> H M Abdul Fattah, Gabriel Gedaliah Geffen, Tanya George, Sunday Usman "
    affiliations:
      - name: "School of Information, University of Arizona"
description: "Project description"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
jupyter: python3
---

## Abstract

This study investigates the relationship between age demographics and severe crashes, with a focus on developing a predictive model to enhance road safety in Massachusetts. Using a crash dataset from January 2024, we explore how age correlates with the severity of crashes and examine environmental factors like lighting, weather, road conditions, speed limits, and the number of vehicles involved. Our analysis reveals crucial patterns, indicating which age groups, both drivers and vulnerable users, are at greater risk of severe crashes. Additionally, we identify environmental conditions that contribute to the likelihood and severity of crashes, providing insights for targeted safety measures. To classify crash severity, we experimented with various machine learning (ML) techniques, including logistic regression, decision trees, random forests, and K Nearest Neighbors (KNN). Our models achieved a 100% prediction accuracy, indicating a strong ability to classify crash severity based on the selected features. However, the absence of road volume or vehicle miles traveled data poses a limitation in contextualizing the frequency of crashes. The outcomes of our research offer valuable tools for policymakers and practitioners, allowing for more proactive safety measures and resource allocation. By accurately predicting crash risks based on age demographics and environmental conditions, authorities can implement preemptive interventions to reduce severe accidents. Ultimately, this study contributes to a data-driven approach to road safety, with the potential to make tangible improvements in public safety and traffic management.

## Introduction

Understanding the factors contributing to severe car crashes is crucial for improving road safety and reducing traffic-related injuries and fatalities. This project aims to develop a predictive model that correlates age demographics with severe crashes in Massachusetts. The ultimate goal is to identify key risk factors and provide data-driven insights for implementing effective safety measures.

Our team is analyzing a comprehensive dataset of car crashes from January 2024, collected from the Massachusetts Registry of Motor Vehicles. This dataset comprises 72 dimensions, encompassing a range of variables, including crash characteristics, driver demographics, environmental conditions, and vehicle information. By examining these variables, we seek to uncover patterns that link age with severe crashes, offering valuable insights into potential high-risk groups and circumstances.

Our analysis focuses on two main research questions: identifying the age groups most at risk for severe crashes and exploring the role of environmental factors such as lighting, weather, road conditions, and speed limits. Additionally, we aim to develop a predictive model capable of classifying crash severity based on these variables. To achieve this, we used multiple binary classification models, which are known for their simplicity and effectiveness in classification tasks.

The methodology for our analysis involved several key steps. First, we pre-processed the dataset to handle missing data, standardize categorical variables, and scale numerical features. Next, we conducted exploratory data analysis to identify significant correlations and patterns. To predict crash severity, we trained a KNN model using a subset of the data and evaluated its performance on a separate test set. The model's accuracy, precision, recall, and F1-score were measured to determine its effectiveness. The high accuracy achieved in the model's predictions indicates its potential for real-world application in road safety.

This report details our approach to analyzing the Massachusetts crash dataset, including the steps taken to process the data, build the predictive model, and evaluate its performance. We discuss our findings and provide insights into which age groups are most at risk, along with the environmental factors that contribute to severe crashes. Through this work, we aim to contribute to road safety practices and provide useful information for policymakers, traffic safety professionals, and other stakeholders interested in reducing traffic-related incidents and enhancing public safety.

## Questions

1. Which age groups are at the highest risk of getting into severe crashes, and how do factors like lighting, weather, road conditions, speed limits, and the number of vehicles involved contribute to the likelihood of certain age groups being in more danger?
2. Is it possible to develop a model that can accurately classify the severity of crashes based on our findings from the previous question about factors that contribute to said level of danger?

## Analysis Plan


```{python}
#| label: load-pkgs
#| echo: false
#| message: false

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import numpy as np
from scipy.stats import chi2_contingency
import warnings
# Ignore all warnings
warnings.filterwarnings("ignore")
```

```{python}
#| label: load-data
#| echo: false

# Read in the data
url = 'data/crash_data.csv'
crash_data = pd.read_csv(url)
```

```{python}
#| label: data-overview
#| echo: false

# Get the count of each data type in the DataFrame
data_type_counts = crash_data.dtypes.value_counts()

print("Count of each data type in the DataFrame:")
print(data_type_counts)
print()

# Display the first few rows to understand the structure of the dataset
crash_data.head()
```

### Question 1

```{python}
#| label: summary-stats-for-numerical-variables
#| echo: false

# Display data summary stats
crash_data.describe()
```

```{python}
#| label: Check-for-missing-values-in-key-columns
#| echo: false

# Check for missing values in key columns
print(crash_data[['Age', 'Light Conditions', 'Weather Conditions', 'Road Surface Condition']].isnull().sum())
```

```{python}
#| label: Handling-missing-and-duplicate-rows-of-key-columns
#| echo: false

# Impute missing values for 'Age of Driver using median
crash_data['Age'].fillna(crash_data['Age'].median(), inplace=True)


# Since the missing values for Light, Weather, and Road Conditions are minimal, we'll impute these with the mode
common_light = crash_data['Light Conditions'].mode()[0]
common_weather = crash_data['Weather Conditions'].mode()[0]
common_road_surface = crash_data['Road Surface Condition'].mode()[0]


crash_data['Light Conditions'].fillna(common_light, inplace=True)
crash_data['Weather Conditions'].fillna(common_weather, inplace=True)
crash_data['Road Surface Condition'].fillna(common_road_surface, inplace=True)


# Confirm changes by checking missing values again
print(crash_data[['Age', 'Light Conditions', 'Weather Conditions', 'Road Surface Condition']].isnull().sum())

```

```{python}
#| label: Define-age-groups-for-easier-analysis
#| echo: false

bins = [0, 18, 25, 40, 55, 70, 100]
labels = ['0-18', '19-25', '26-40', '41-55', '56-70', '71+']
crash_data['Age Group'] = pd.cut(crash_data['Age'], bins=bins, labels=labels, right=False)

```

```{python}
#| label: Visualization-of-age-group-and-crash-severity
#| echo: false

plt.figure(figsize=(12, 6))
sns.countplot(x='Age Group', hue='Crash Severity', data=crash_data, palette='coolwarm')
plt.title('Crash Severity Distribution by Driver Age Group')
plt.xlabel('Age Group  Driver')
plt.ylabel('Number of Crashes')
plt.legend(title='Severity')
plt.show()

```

```{python}
#| label: Visualizations-for-crash-severity-and-light-conditions
#| echo: false

plt.figure(figsize=(14, 7))
sns.countplot(x='Light Conditions', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Light Conditions')
plt.xlabel('Light Conditions')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=45)
plt.show()


```

```{python}
#| label: Heatmap-of-lighting-affecting-severity-of-danger-by-age-groups
#| echo: false

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 25:
        return '20-25'
    elif 25 <= age <= 35:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Light Conditions', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)
plt.figure(figsize=(10, 6))

sns.heatmap(norm_pivot, annot=True, fmt=".2f", linewidths=.5, cmap='plasma')
plt.title('Heatmap of how lighting plays a factor for certain age groups being in more danger:')
plt.xlabel('Light Conditions')
plt.ylabel('Age Group')
plt.show()

```

```{python}
#| label: Visualizations-for-crash-severity-and-weather-Conditions
#| echo: false

plt.figure(figsize=(14, 7))
sns.countplot(x='Weather Conditions', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Weather Conditions')
plt.xlabel('Weather Conditions')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=45) 
plt.show()


```

```{python}
#| label: Heatmap-for-weather-by-severity-and-age
#| echo: false

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 45:
        return '20-25'
    elif 45 <= age <= 60:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

# Categorizing weather conditions
def weathergroups(weather):
    if weather in ['Blowing sand, snow', 'Blowing sand, snow/Snow', 'Clear/Snow', 'Cloudy/Blowing sand, snow', 'Other/Snow']:
        return 'Snowy conditions'
    elif weather in ['Clear/Rain', 'Fog, smog, smoke/Rain', 'Rain/Blowing sand, snow', 'Rain/Fog, smog, smoke', 'Rain/Severe crosswinds', 'Rain/Unknown']:
        return 'Rainy/foggy conditions'
    elif weather in ['Snow/Clear', 'Snow/Rain', 'Snow/Snow']:
        return 'Mostly Snowy'
    elif weather in ['Clear/Clear', 'Unknown/Clear']:
        return 'Clear weather'
    elif weather in ['Cloudy/Blowing sand, snow', 'Cloudy/Fog, smog, smoke', 'Cloudy/Severe crosswinds', 'Cloudy/Unknown']:
        return 'Cloudy/smog conditions'
    elif weather in ['Sleet, hail(freezing rain or drizzle)', 'Sleet, hail(freezing rain or drizzle)/Fog, smog, smoke', 'Sleet, hail(freezing rain or drizzle)/Severe crosswinds', 'Sleet, hail(freezing rain or drizzle)/Unknown']:
        return 'Hail/Freezing Drizzle'


crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

crash_data['Weather Group'] = crash_data['Weather Conditions'].apply(weathergroups)


# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Weather Group', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)

plt.figure(figsize=(10, 6))
sns.heatmap(norm_pivot, cmap='plasma', annot=True, fmt=".2f", linewidths=.5)
plt.title('Heatmap of how weather plays a factor for certain age groups being in more danger:')
plt.xlabel('Weather Conditions')
plt.ylabel('Age Group')
plt.show()

```

```{python}
#| label: Visualizations-for-crash-severity-and-road-surface-conditions
#| echo: false

plt.figure(figsize=(7, 4))
sns.countplot(x='Road Surface Condition', hue='Crash Severity', data=crash_data, palette='viridis')
plt.title('Crash Severity by Road Surface Condition')
plt.xlabel('Road Surface Condition')
plt.ylabel('Number of Crashes')
plt.legend(title='Crash Severity')
plt.xticks(rotation=60) 
plt.show()


```

```{python}
#| label: Heatmap-for-road-surface-condition-by-severity-and-age
#| echo: false

# Categorizing age groups
def agegroups(age):
    if age < 20:
        return 'Under 20'
    elif 20 <= age <= 25:
        return '20-25'
    elif 25 <= age <= 35:
        return '25-35'
    elif 35 <= age <= 50:
        return '35-50'    
    else:
        return '60 and above'

crash_data['Age Group'] = crash_data['Age'].apply(agegroups)

# summarizing data using a pivot table
pivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Road Surface Condition', aggfunc='count')

# Normalize pivot table
norm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)
plt.figure(figsize=(10, 6))

sns.heatmap(norm_pivot, annot=True, fmt=".2f", linewidths=.5, cmap='plasma')
plt.title('Heatmap of how Road surfaces play a factor for certain age groups being in more danger:')
plt.xlabel('Road Surface Condition')
plt.ylabel('Age Group')
plt.show()

```

```{python}
#| label: Visualizations-for-number-of-crashes-by-age-and-Light-Conditions

plt.figure(figsize=(9, 6))
sns.countplot(x='Age Group', hue='Light Conditions', data=crash_data, palette='coolwarm')
plt.title('Impact of Light Conditions on number of crashes by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Number of Crashes')
plt.legend(title='Light Conditions')
plt.show()


```

```{python}
#| label: testing association between Light Conditions and Crash Severity within an Age Group


#contingency_table = pd.crosstab(index=crash_data[crash_data['Age Group'] #== '19-25']['Crash Severity'],
                               #columns=crash_data[crash_data['Age Group'] #== '19-25']['Light Conditions'])
#chi2, p_value, dof, expected = chi2_contingency(contingency_table)
#print(f"Chi-square test p-value for 19-25 age group and light conditions: #{p_value}")


```

### Question 2:

The initial analysis from question 1 yielded interesting insights into the relationship between age and crash severity, along with environmental factors like lighting, weather, and road conditions. These findings help identify which age groups are most at risk and the circumstances that contribute to severe crashes. Given these insights, we now move to question 2, where the goal is to create a predictive model to classify crash severity.

To start, we need to preprocess the crash data by filtering out rows where the severity is unknown. Next, we create a binary variable to distinguish crashes with "no injury" (property damage only) from those involving injuries or fatalities. This step is crucial due to the heavy imbalance of fatal crashes, which are relatively rare. This binary classification allows for a more straightforward modeling approach, focusing on predicting the likelihood of crashes resulting in injury or fatality. Below, we create a table to display the count of no-injury crashes and injury/fatality crashes to understand the distribution of our target variable.

```{python}
#| label: Create-binary-feature-variable
#| code-fold: true


# Filter rows where the severity is unknow
crash_data = crash_data[crash_data['Crash Severity'] != "Unknown"]

# Add a new column named 'feature_variable'
crash_data['feature_variable'] = [0 if x == 'Property damage only (none injured)' else 1 for x in crash_data['Crash Severity']]

# Drop the 'Crash Severity' column
crash_data = crash_data.drop('Crash Severity', axis=1)

# Create a count table for the new feature variable
severity_counts = crash_data['feature_variable'].value_counts().rename({0: 'No Injury', 1: 'Injury/Fatality'})

# Display the count table
print(severity_counts)
```

```{python}
#| label: visualize-relationship-between-feature-and-target-variable
#| echo: false

# Instead of using all the features, I am using custom_feature_set 
# for simplicity with 3 most correlated features and 2 least correlated features

numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64'])

correlation_matrix = numerical_cols.corr()

# The correlation of each feature with the target variable 'mc_preschool'
correlation_with_target = correlation_matrix['feature_variable'].abs().sort_values(ascending=False)

most_correlated_features = correlation_with_target[1:4]  # Exclude the target variable itself
least_correlated_features = correlation_with_target[-2:] 

# Convert the indices of selected features to a list and merge them together
most_correlated_features = most_correlated_features.index.tolist()
least_correlated_features = least_correlated_features.index.tolist()
custom_feature_set = most_correlated_features + least_correlated_features

# Visualize relationships between the custom feature set and the target variable using a pairplot
sns.pairplot(numerical_cols, x_vars=custom_feature_set, y_vars='feature_variable', kind='scatter')
plt.show()

```

```{python}
#| label: Data-cleaning-and-missing-value-analysis
#| echo: false

# Find numerical columns
numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64'])

# Calculate missing values count for each numerical column
missing_values_count = numerical_cols.isnull().sum()

# Calculate missing rate for each numerical column
missing_rate = (missing_values_count / len(crash_data)) * 100

missing_data = pd.DataFrame({
    'Missing Values': missing_values_count,
    'Percentage (%)': missing_rate
})

print('Analysis of Missing Values for numerical features: \n\n', missing_data, '\n\n')

# Drop categorical columns with missing rate over 50%
columns_to_drop = missing_rate[missing_rate > 50].index
crash_data = crash_data.drop(columns_to_drop, axis=1)

# Find categorical columns
categorical_columns = crash_data.select_dtypes(include = ['object', 'category'])

# Calculate missing values count for each categorical column
missing_values_count = categorical_columns.isnull().sum()

# Calculate missing rate for each categorical column
missing_rate = (missing_values_count / len(crash_data)) * 100

missing_data = pd.DataFrame({
    'Missing Values': missing_values_count,
    'Percentage (%)': missing_rate
})

print('Analysis of Missing Values for categorical features: \n\n', missing_data, '\n\n')

# Drop categorical columns with missing rate over 50%
columns_to_drop = missing_rate[missing_rate > 50].index
crash_data = crash_data.drop(columns_to_drop, axis=1)
```

```{python}
#| label: Missing-value-imputation
#| echo: false

all_columns = crash_data.select_dtypes(include = ['int64', 'float64', 'object', 'category'])

# Calculate mode for each categorical column
mode_values = all_columns.mode().iloc[0]

# Fill missing values with mode for each column
crash_data = crash_data.fillna(mode_values)

print(crash_data.head())
```

```{python}
#| label: Standardize-features-with-StandardScaler
#| echo: false

numerical_cols = crash_data.select_dtypes(include = ['int64', 'float64']).columns

# Exclude our output feature: 'feature_variable'
feature_to_exclude = 'feature_variable'
numerical_cols = numerical_cols.drop(feature_to_exclude, errors='ignore')

scaler = StandardScaler()

# Fit and transform the numerical columns using StandardScaler
crash_data[numerical_cols] = scaler.fit_transform(crash_data[numerical_cols])

# Display the first few rows of the scaled dataset
print(crash_data.head())
```

```{python}
#| label: Encoding-categorical-variables
#| echo: false

# Encode categorical variables
categorical_columns = crash_data.select_dtypes(include = ['object', 'category']).columns.tolist()

# Convert numerical columns to strings if necessary
crash_data[categorical_columns] = crash_data[categorical_columns].astype(str)

# Encode categorical variables
label_encoders = {col: LabelEncoder() for col in categorical_columns}
for col in categorical_columns:
    crash_data[col] = label_encoders[col].fit_transform(crash_data[col])

# Convert categorical columns to numerical using one-hot encoding
crash_data = pd.get_dummies(crash_data, columns=categorical_columns)

print(crash_data.head())
```

```{python}
# Define features and target
X = crash_data.drop('feature_variable', axis = 1)
y = crash_data['feature_variable']
```

```{python}
batch_size = 100

# Initialize an empty array to store the selected feature indices
selected_feature_indices = []

# Iterate over the dataset in batches and perform feature selection
for batch_start in range(0, len(X), batch_size):
    X_batch = X.iloc[batch_start:batch_start+batch_size]
    y_batch = y.iloc[batch_start:batch_start+batch_size]
    
    # Perform feature selection on the current batch
    k_best_selector = SelectKBest(score_func=f_classif, k=30)
    X_selected_batch = k_best_selector.fit_transform(X_batch, y_batch)
    
    # Get the indices of the selected features and append to the list
    selected_feature_indices.extend(k_best_selector.get_support(indices=True))

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

print('Selected features: \n\n', selected_features)

# Extract the selected features from the original DataFrame X
X_selected = X[selected_features]
```

```{python}
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Display the shapes of the training and testing sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)
```

```{python}
log_reg = LogisticRegression(solver = 'liblinear', max_iter = 1000, random_state = 42)
log_reg.fit(X_train, y_train)

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)

rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_train, y_train)

knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)
```

```{python}
# Model Validation

# List of classifiers
classifiers = [log_reg, dtree, rf_classifier, knn]
# classifiers = [log_reg]

# Perform cross-validation and compute evaluation metrics for each classifier
for classifier in classifiers:
    # Cross-validation
    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)

    # Compute evaluation metrics
    accuracy = cv_scores.mean()
    precision = precision_score(y_test, classifier.predict(X_test))
    recall = recall_score(y_test, classifier.predict(X_test))
    f1 = f1_score(y_test, classifier.predict(X_test))
    
    # Print the results
    print('Classifier: ', str(classifier))
    print('Accuracy: ', accuracy)
    print('Precision: ', precision)
    print('Recall: ', recall)
    print('F1-Score: ', f1)
```

```{python}
probs_knn = knn.predict_proba(X_test)
# Keep probabilities for the positive outcome only
probs_knn = probs_knn[:, 1]
# Compute the ROC curve
fpr_knn, tpr_knn, thresholds = roc_curve(y_test, probs_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

probs_forest = rf_classifier.predict_proba(X_test)
probs_forest = probs_forest[:, 1]
fpr_forest, tpr_forest, thresholds = roc_curve(y_test, probs_forest)
roc_auc_forest = auc(fpr_forest, tpr_forest)


probs_tree = dtree.predict_proba(X_test)
probs_tree = probs_tree[:, 1]
fpr_tree, tpr_tree, thresholds = roc_curve(y_test, probs_tree)
roc_auc_tree = auc(fpr_tree, tpr_tree)


probs_log = log_reg.predict_proba(X_test)
probs_log = probs_log[:, 1]
fpr_log, tpr_log, thresholds = roc_curve(y_test, probs_log)
roc_auc_log = auc(fpr_log, tpr_log)
```

```{python}
plt.figure(figsize=(8, 6))
plt.plot(fpr_knn, tpr_knn, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_knn:.2f}) for KNN')
plt.plot(fpr_tree, tpr_tree, color='blue', lw=2, label=f'ROC curve (area = {roc_auc_tree:.2f}) for Decision Tree')
plt.plot(fpr_forest, tpr_forest, color='red', lw=2, label=f'ROC curve (area = {roc_auc_forest:.2f}) for Random Forest')
plt.plot(fpr_log, tpr_log, color='Green', lw=2, label=f'ROC curve (area = {roc_auc_log:.2f}) for Logistic Regression')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal 45 degree line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

```{python}
### Accuracy on Test data

predictions_log = log_reg.predict(X_test)
print("Logistic Regression Accuracy:", np.round(accuracy_score(y_test, predictions_log),3))

predictions_tree = dtree.predict(X_test)
print("Decision Tree Accuracy:", np.round(accuracy_score(y_test, predictions_tree),3))

predictions_forest = rf_classifier.predict(X_test)
print("Random Forest Accuracy:", np.round(accuracy_score(y_test, predictions_forest),3))


predictions_knn = knn.predict(X_test)
print("KNN Accuracy:",np.round(accuracy_score(y_test, predictions_knn),3))


```

```{python}
## Confusion Matrix
cm = confusion_matrix(y_test, predictions_log)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Logistic Regression Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()
```

```{python}
## Confusion Matrix
cm = confusion_matrix(y_test, predictions_knn)
sns.heatmap(cm, annot = True, fmt ='g')
plt.title('KNN Confusion Matrix') 
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

```

```{python}
cm = confusion_matrix(y_test, predictions_tree)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Decision Tree Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

```

```{python}

cm = confusion_matrix(y_test, predictions_forest)
sns.heatmap(cm, annot = True, fmt = 'g')
plt.title('Random Forest Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()
```
