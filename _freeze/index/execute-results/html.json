{
  "hash": "c3bf8e4fbc66f1bdfd9e25d45461f03d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: ANALYTICAL AVENGERS\nsubtitle: INFO 523 - Project Final\nauthor:\n  - name: 'ANALYTICAL AVENGER:-<br> Melika Akbarsharifi, Divya liladhar Dhole, Mohammad Ali Farmani,<br> H M Abdul Fattah, Gabriel Gedaliah Geffen, Tanya George, Sunday Usman '\n    affiliations:\n      - name: 'School of Information, University of Arizona'\ndescription: Project description\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n---\n\n## Abstract\n\nThis study investigates the relationship between age demographics and severe crashes, with a focus on developing a predictive model to enhance road safety in Massachusetts. Using a crash dataset from January 2024, we explore how age correlates with the severity of crashes and examine environmental factors like lighting, weather, road conditions, speed limits, and the number of vehicles involved. Our analysis reveals crucial patterns, indicating which age groups, both drivers and vulnerable users, are at greater risk of severe crashes. Additionally, we identify environmental conditions that contribute to the likelihood and severity of crashes, providing insights for targeted safety measures. To classify crash severity, we experimented with various machine learning (ML) techniques, including logistic regression, decision trees, random forests, and K Nearest Neighbors (KNN). Our models achieved a prediction accuracy of around 78% in all cases, indicating a strong ability to classify crash severity based on the selected features. However, the absence of road volume or vehicle miles traveled data poses a limitation in contextualizing the frequency of crashes. The outcomes of our research offer valuable tools for policymakers and practitioners, allowing for more proactive safety measures and resource allocation. By accurately predicting crash risks based on age demographics and environmental conditions, authorities can implement preemptive interventions to reduce severe accidents. Ultimately, this study contributes to a data-driven approach to road safety, with the potential to make tangible improvements in public safety and traffic management.\n\n## Introduction\n\nUnderstanding the factors contributing to severe car crashes is crucial for improving road safety and reducing traffic-related injuries and fatalities. This project aims to develop a predictive model that correlates age demographics with severe crashes in Massachusetts. The ultimate goal is to identify key risk factors and provide data-driven insights for implementing effective safety measures.\n\nOur team is analyzing a comprehensive dataset of car crashes from January 2024, collected from the Massachusetts Registry of Motor Vehicles. This dataset comprises 72 dimensions, encompassing a range of variables, including crash characteristics, driver demographics, environmental conditions, and vehicle information. By examining these variables, we seek to uncover patterns that link age with severe crashes, offering valuable insights into potential high-risk groups and circumstances.\n\nOur analysis focuses on two main research questions: identifying the age groups most at risk for severe crashes and exploring the role of environmental factors such as lighting, weather, road conditions, and speed limits. Additionally, we aim to develop a predictive model capable of classifying crash severity based on these variables. To achieve this, we used multiple binary classification models, which are known for their simplicity and effectiveness in classification tasks.\n\nThe methodology for our analysis involved several key steps. First, we pre-processed the dataset to handle missing data, standardize categorical variables, and scale numerical features. Next, we conducted exploratory data analysis to identify significant correlations and patterns. To predict crash severity, we trained a KNN model using a subset of the data and evaluated its performance on a separate test set. The model's accuracy, precision, recall, and F1-score were measured to determine its effectiveness. The high accuracy achieved in the model's predictions indicates its potential for real-world application in road safety.\n\nThis report details our approach to analyzing the Massachusetts crash dataset, including the steps taken to process the data, build the predictive model, and evaluate its performance. We discuss our findings and provide insights into which age groups are most at risk, along with the environmental factors that contribute to severe crashes. Through this work, we aim to contribute to road safety practices and provide useful information for policymakers, traffic safety professionals, and other stakeholders interested in reducing traffic-related incidents and enhancing public safety.\n\n## Questions\n\n1. Which age groups are at the highest risk of getting into severe crashes, and how do factors like lighting, weather, road conditions, speed limits, and the number of vehicles involved contribute to the likelihood of certain age groups being in more danger?\n2. Is it possible to develop a model that can accurately classify the severity of crashes based on our findings from the previous question about factors that contribute to said level of danger?\n\n## Analysis Plan\n\nAs with any data analysis, the first step involves loading the necessary packages and importing the dataset. This ensures that all required tools and resources are available for the subsequent analysis. The output below displays the various data types in our dataset, providing a comprehensive overview of the features at our disposal, thanks to the Massachusetts Department of Transportation (MassDOT).\n\nTo get a better understanding of our data, we examine the count of each data type to identify the composition of our dataset, including numerical, categorical, and text-based features. Additionally, we present the first few rows of the dataset (the \"head\") to give an initial overview of its structure and content. This initial exploration helps set the stage for further data processing, cleaning, and analysis, ensuring that we start with a clear understanding of the dataset's characteristics and layout.\n\n\n\n\n\n::: {#cell-data-overview .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nCount of each data type in the DataFrame:\nobject     59\nfloat64    13\ndtype: int64\n\n```\n:::\n\n::: {#data-overview .cell-output .cell-output-display execution_count=115}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Crash Number</th>\n      <th>City Town Name</th>\n      <th>Crash Date</th>\n      <th>Crash Severity</th>\n      <th>Crash Status</th>\n      <th>Crash Time</th>\n      <th>Crash Year</th>\n      <th>Max Injury Severity Reported</th>\n      <th>Number of Vehicles</th>\n      <th>Police Agency Type</th>\n      <th>...</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Vehicle Unit Number</th>\n      <th>Vehicle Make</th>\n      <th>Vehicle Model</th>\n      <th>Person Number</th>\n      <th>Age</th>\n      <th>Sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5342297</td>\n      <td>LOWELL</td>\n      <td>01/01/2024</td>\n      <td>Non-fatal injury</td>\n      <td>Open</td>\n      <td>3:26 AM</td>\n      <td>2024.0</td>\n      <td>Possible Injury (C)</td>\n      <td>1.0</td>\n      <td>Local police</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>HOND</td>\n      <td>HR-V</td>\n      <td>1.0</td>\n      <td>32.0</td>\n      <td>F - Female</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5342292</td>\n      <td>LOWELL</td>\n      <td>01/01/2024</td>\n      <td>Property damage only (none injured)</td>\n      <td>Open</td>\n      <td>12:48 AM</td>\n      <td>2024.0</td>\n      <td>No Apparent Injury (O)</td>\n      <td>2.0</td>\n      <td>Local police</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NISS</td>\n      <td>ALTIMA</td>\n      <td>1.0</td>\n      <td>60.0</td>\n      <td>M - Male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5342292</td>\n      <td>LOWELL</td>\n      <td>01/01/2024</td>\n      <td>Property damage only (none injured)</td>\n      <td>Open</td>\n      <td>12:48 AM</td>\n      <td>2024.0</td>\n      <td>No Apparent Injury (O)</td>\n      <td>2.0</td>\n      <td>Local police</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>HOND</td>\n      <td>ACCORD</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5342292</td>\n      <td>LOWELL</td>\n      <td>01/01/2024</td>\n      <td>Property damage only (none injured)</td>\n      <td>Open</td>\n      <td>12:48 AM</td>\n      <td>2024.0</td>\n      <td>No Apparent Injury (O)</td>\n      <td>2.0</td>\n      <td>Local police</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>HOND</td>\n      <td>ACCORD</td>\n      <td>3.0</td>\n      <td>31.0</td>\n      <td>M - Male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5342292</td>\n      <td>LOWELL</td>\n      <td>01/01/2024</td>\n      <td>Property damage only (none injured)</td>\n      <td>Open</td>\n      <td>12:48 AM</td>\n      <td>2024.0</td>\n      <td>No Apparent Injury (O)</td>\n      <td>2.0</td>\n      <td>Local police</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>HOND</td>\n      <td>ACCORD</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>M - Male</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 72 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Question 1\n\nTo address Question 1, the analysis begins with a detailed examination of the 13 float variables identified in the previous section. The first step involves using the *'.describe()'* method to generate initial summary statistics for these variables. This provides a quick overview of the data distribution, central tendencies, and dispersion, which is essential for understanding the basic characteristics of the numerical features.\n\nThe summary statistics include key metrics such as mean, median, standard deviation, minimum and maximum values, and quartiles. By analyzing these statistics, we can identify potential outliers, skewness, and other characteristics that may influence subsequent analysis. This foundational step allows us to assess the general trends and variations within the float variables, offering insights into how they may relate to the target variable and other categorical features in the dataset.\n\n::: {#cell-summary-stats-for-numerical-variables .cell execution_count=4}\n\n::: {#summary-stats-for-numerical-variables .cell-output .cell-output-display execution_count=116}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Crash Year</th>\n      <th>Number of Vehicles</th>\n      <th>MassDOT District</th>\n      <th>Total Fatalities</th>\n      <th>Total Non-Fatal Injuries</th>\n      <th>Speed Limit</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Vehicle Unit Number</th>\n      <th>Person Number</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>25547.0</td>\n      <td>25547.000000</td>\n      <td>25547.000000</td>\n      <td>25547.000000</td>\n      <td>25547.000000</td>\n      <td>23389.000000</td>\n      <td>21002.000000</td>\n      <td>21002.000000</td>\n      <td>20823.000000</td>\n      <td>20823.000000</td>\n      <td>25220.000000</td>\n      <td>25547.000000</td>\n      <td>23002.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2024.0</td>\n      <td>1.976749</td>\n      <td>4.019063</td>\n      <td>0.003562</td>\n      <td>0.318824</td>\n      <td>34.394502</td>\n      <td>205930.128516</td>\n      <td>887470.383156</td>\n      <td>42.234940</td>\n      <td>-71.431249</td>\n      <td>1.489968</td>\n      <td>1.918699</td>\n      <td>38.952265</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>0.702530</td>\n      <td>1.325421</td>\n      <td>0.068730</td>\n      <td>0.728140</td>\n      <td>12.979679</td>\n      <td>49539.383540</td>\n      <td>31782.135543</td>\n      <td>0.287058</td>\n      <td>0.600959</td>\n      <td>0.637851</td>\n      <td>1.568750</td>\n      <td>18.503512</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2024.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>44708.708525</td>\n      <td>779050.104521</td>\n      <td>41.251611</td>\n      <td>-73.386241</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2024.0</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>179154.370652</td>\n      <td>870946.937400</td>\n      <td>42.086592</td>\n      <td>-71.756001</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>24.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2024.0</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>30.000000</td>\n      <td>224092.943601</td>\n      <td>889548.926635</td>\n      <td>42.254041</td>\n      <td>-71.209095</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>36.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2024.0</td>\n      <td>2.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>237299.607076</td>\n      <td>908937.437400</td>\n      <td>42.428108</td>\n      <td>-71.049485</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2024.0</td>\n      <td>9.000000</td>\n      <td>6.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>65.000000</td>\n      <td>327948.082270</td>\n      <td>958417.191000</td>\n      <td>42.874973</td>\n      <td>-69.962834</td>\n      <td>9.000000</td>\n      <td>42.000000</td>\n      <td>99.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAs part of the analysis plan for Question 1, the next step involves identifying missing values and duplicate rows in the dataset. Given that the question focuses on age groups at the highest risk of severe crashes and the factors that contribute to crash severity, it's crucial to ensure the data's completeness and consistency.\n\nTo examine the missing data, we check for missing values in the following columns, which are directly related to the question: 'Age', 'Light Conditions', 'Weather Conditions', and 'Road Surface Condition'. Any missing values in these columns could affect the analysis, as they are critical in determining the conditions under which severe crashes occur and the age groups most likely to be involved.\n\n::: {#check-for-missing-values-in-key-columns .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nAge                       2548\nLight Conditions             3\nWeather Conditions           3\nRoad Surface Condition       3\ndtype: int64\n```\n:::\n:::\n\n\n\n\nIn dealing with missing values, we apply different imputation strategies depending on the column type and context. For the 'Light Conditions', 'Weather Conditions', and 'Road Surface Condition' columns, which are categorical, mode imputation is used to fill in missing values. Mode imputation replaces missing entries with the most frequently occurring value, ensuring that the most common data pattern is retained without introducing significant bias.\n\nFor the 'Age' column, which is numerical, median imputation is employed. The median provides a robust measure of central tendency, less susceptible to outliers compared to the mean. This approach is particularly useful when dealing with skewed data or avoiding distortions from extreme values.\n\nIn question 2, which involves building machine learning models, we opt to filter out rows with missing values to avoid biasing the model. However, for this current analysis, mode and median imputation are applied to maintain the dataset's size and continuity. Imputation is chosen here to preserve the context and integrity of the data, allowing for a more comprehensive analysis of crash-related factors.\n\n\n\nFollowing imputation, the 'Age' column is binned into age groups based on the age ranges provided by MassDOT. This transformation is crucial for analyzing the distribution of crash severity across different age groups. Our first visualization is a bar plot displaying the relationship between age group and crash severity, using 'Crash Severity' as the data source. This plot provides a clear visual representation of how crash severity is distributed across age groups, helping to identify patterns or trends that could inform further analysis and safety recommendations.\n\n::: {#cell-Visualization-of-age-group-and-crash-severity .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Replace 'Property damage only (none injured)' with 'No injury'\ncrash_data['Crash Severity'].replace('Property damage only (none injured)', 'No injury', inplace=True)\n\n# Plot with rotated x-axis labels\nplt.figure(figsize=(8, 6))  # Set plot size\nsns.countplot(x='Age Group', hue='Crash Severity', data=crash_data, palette='coolwarm')  # Plot with seaborn\nplt.title('Crash Severity Distribution by Driver Age Group')  # Set title\nplt.xlabel('Age Group Driver')  # Set x-axis label\nplt.ylabel('Number of Crashes')  # Set y-axis label\nplt.xticks(rotation=45)  # Rotate x-axis labels\nplt.legend(title='Crash Severity')  # Set legend title\nplt.show()  # Display the plot\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualization-of-age-group-and-crash-severity-output-1.png){#visualization-of-age-group-and-crash-severity width=676 height=547}\n:::\n:::\n\n\nThe bar plot displaying the distribution of crashes by age group shows a roughly normal distribution, suggesting that crash frequency generally increases with age and then tapers off at older ages. This pattern is consistent across the overall number of crashes and when broken down by individual crash severities.\n\nHowever, one significant observation is the clear imbalance in the data, with a disproportionately high number of crashes classified as \"no-injury\" compared to other severity levels. This imbalance can impact subsequent analyses, as the majority of crashes fall into this less severe category, potentially overshadowing more critical, severe crash cases. This insight underscores the importance of addressing data imbalance when building predictive models or drawing conclusions from the data.\n\n::: {#cell-Visualizations-for-crash-severity-and-light-conditions .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\n# Replace longer labels with shorter ones\ncrash_data['Light Conditions'].replace('Dark - unknown roadway lighting', 'Dark - unknown lighting', inplace=True)\ncrash_data['Light Conditions'].replace('Dark - roadway not lighted', 'Dark - no lighting', inplace=True)\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Light Conditions', hue='Crash Severity', data=crash_data, palette='coolwarm')\nplt.title('Crash Severity by Light Conditions')\nplt.xlabel('Light Conditions')\nplt.ylabel('Number of Crashes')\nplt.legend(title='Crash Severity')\nplt.xticks(rotation=75)\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualizations-for-crash-severity-and-light-conditions-output-1.png){#visualizations-for-crash-severity-and-light-conditions width=685 height=668}\n:::\n:::\n\n\nThe analysis of crash occurrences by light conditions reveals that daylight is the most common setting for crashes. This is unsurprising, as most drivers are on the road during daylight hours, commuting to work, school, or running errands. The higher traffic volumes during these times naturally lead to more accidents.\n\nFollowing daylight, the next most common light condition for crashes is \"dark-lighted roadway.\" This observation is consistent with the typical layout of urban and suburban areas where streetlights are more prevalent, providing better visibility at night. In contrast, rural areas with fewer lighted roadways tend to have less traffic, contributing to fewer overall crashes.\n\nOnce again, the data shows a noticeable imbalance in crash severity. The majority of crashes fall into the \"no-injury\" category, indicating that while accidents are more frequent during daylight and on lighted roadways, they are generally less severe. This recurring pattern of severity imbalance suggests that even as crash frequency fluctuates with light conditions, the majority remain relatively minor in nature.\n\n::: {#cell-Heatmap-of-lighting-affecting-severity-of-danger-by-age-groups .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Create a pivot table to summarize data\npivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', \n                             columns='Light Conditions', aggfunc='count')\n\n# Normalize the pivot table by row (to show proportions across light conditions)\nnorm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\nheatmap = sns.heatmap(norm_pivot, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar=True)\nplt.xticks(rotation=75)  # Rotate x-axis tick labels\nplt.yticks(rotation=45)  # Rotate y-axis tick labels \nplt.title('Heatmap of Crash Severity by Age Group and Light Conditions')\nplt.xlabel('Light Conditions')  # Label for x-axis\nplt.ylabel('Age Group')  # Label for y-axis\ncbar = heatmap.collections[0].colorbar  # Get the colorbar\ncbar.set_label('Proportion of Crash Severity')  # Indicate proportion of crash types within a group\nplt.show()  # Display the heatmap\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/heatmap-of-lighting-affecting-severity-of-danger-by-age-groups-output-1.png){#heatmap-of-lighting-affecting-severity-of-danger-by-age-groups width=787 height=668}\n:::\n:::\n\n\nExamining the heatmap of crash severity by age group and light conditions, viewed as a proportion rather than a total count, reveals some intriguing insights. This approach allows us to better understand the relative distribution of crash severities within each category, offering a nuanced perspective on the factors contributing to different types of crashes.\n\nThe heatmap indicates that the most common age groups and lighting conditions tend to have the highest proportion of no-injury crashes. This observation suggests that higher vehicle volumes, often associated with daytime driving, result in more crashes overall, but these tend to be less severe. A plausible explanation is that during daytime, increased traffic volumes lead to more minor collisions due to congestion and low-speed accidents, which are generally safer.\n\nAdditionally, the data shows that older people are significantly more likely to be involved in crashes during daylight hours, with a higher proportion of no-injury crashes. This trend aligns with typical driving patterns, where older drivers are less likely to drive at night. This finding may also reflect safer driving behavior among older drivers, who tend to avoid risky conditions such as nighttime driving.\n\n::: {#cell-Visualizations-for-crash-severity-and-weather-Conditions .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# Mapping from original weather conditions to simplified categories\nweather_mapping = {\n    # Clear weather\n    \"Clear\": \"Clear\",\n    \"Clear/Clear\": \"Clear\",\n    \"Clear/Cloudy\": \"Clear\",\n    \"Clear/Other\": \"Clear\",\n    \"Clear/Unknown\": \"Clear\",\n    \"Clear/Snow\": \"Clear\",\n    \"Clear/Rain\": \"Clear\",\n    \"Clear/Blowing sand, snow\": \"Clear\",\n\n    # Cloudy weather\n    \"Cloudy\": \"Cloudy\",\n    \"Cloudy/Cloudy\": \"Cloudy\",\n    \"Cloudy/Clear\": \"Cloudy\",\n    \"Cloudy/Unknown\": \"Cloudy\",\n    \"Cloudy/Other\": \"Cloudy\",\n    \"Cloudy/Blowing sand, snow\": \"Cloudy\",\n    \"Cloudy/Fog, smog, smoke\": \"Cloudy\",\n    \n    # Rain\n    \"Rain\": \"Rain\",\n    \"Rain/Rain\": \"Rain\",\n    \"Rain/Cloudy\": \"Rain\",\n    \"Rain/Sleet, hail (freezing rain or drizzle)\": \"Rain\",\n    \"Rain/Fog, smog, smoke\": \"Rain\",\n    \"Rain/Severe crosswinds\": \"Rain\",\n    \"Rain/Other\": \"Rain\",\n    \"Rain/Unknown\": \"Rain\",\n    \n    # Snow\n    \"Snow\": \"Snow\",\n    \"Snow/Snow\": \"Snow\",\n    \"Snow/Cloudy\": \"Snow\",\n    \"Snow/Clear\": \"Snow\",\n    \"Snow/Rain\": \"Snow\",\n    \"Snow/Other\": \"Snow\",\n    \"Snow/Blowing sand, snow\": \"Snow\",\n    \"Snow/Sleet, hail (freezing rain or drizzle)\": \"Snow\",\n    \n    # Sleet, hail\n    \"Sleet, hail (freezing rain or drizzle)\": \"Sleet/Hail\",\n    \"Sleet, hail (freezing rain or drizzle)/Snow\": \"Sleet/Hail\",\n    \"Sleet, hail (freezing rain or drizzle)/Cloudy\": \"Sleet/Hail\",\n    \"Sleet, hail (freezing rain or drizzle)/Severe crosswinds\": \"Sleet/Hail\",\n    \"Sleet, hail (freezing rain or drizzle)/Blowing sand, snow\": \"Sleet/Hail\",\n    \"Sleet, hail (freezing rain or drizzle)/Fog, smog, smoke\": \"Sleet/Hail\",\n    \n    # Severe crosswinds and windy conditions\n    \"Severe crosswinds\": \"Windy\",\n    \"Blowing sand, snow\": \"Windy\",\n    \n    # Fog, smog, smoke\n    \"Fog, smog, smoke\": \"Fog\",\n    \"Fog, smog, smoke/Cloudy\": \"Fog\",\n    \"Fog, smog, smoke/Rain\": \"Fog\",\n    \n    # Other and Unknown\n    \"Unknown\": \"Unknown\",\n    \"Unknown/Unknown\": \"Unknown\",\n    \"Not Reported\": \"Unknown\",\n    \"Other\": \"Other\",\n    \"Reported but invalid\": \"Other\",\n    \"Unknown/Clear\": \"Unknown\",\n    \"Unknown/Other\": \"Unknown\",\n}\n\n# Apply the mapping to simplify the \"Weather Conditions\"\ncrash_data[\"Weather Conditions\"] = crash_data[\"Weather Conditions\"].map(weather_mapping).fillna(\"Other\")\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Weather Conditions', hue='Crash Severity', data=crash_data, palette='coolwarm')\nplt.title('Crash Severity by Weather Conditions')\nplt.xlabel('Weather Conditions')\nplt.ylabel('Number of Crashes')\nplt.legend(title='Crash Severity')\nplt.xticks(rotation=45) \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualizations-for-crash-severity-and-weather-conditions-output-1.png){#visualizations-for-crash-severity-and-weather-conditions width=685 height=565}\n:::\n:::\n\n\nAfter filtering and simplifying the weather conditions to six main categories, we can analyze their impact on crash occurrences and severity. As expected, clear weather conditions are associated with the highest number of crashes, and, unsurprisingly, \"no injury\" is the most common outcome. This pattern aligns with general expectations, as most driving occurs during clear weather, with higher traffic volumes leading to more minor accidents.\n\nInterestingly, the data reveals that snowy conditions are associated with more crashes than cloudy weather, despite cloudy weather likely being more common. This observation suggests that snowy conditions, which often reduce visibility and traction, could increase the likelihood of accidents, even if the overall frequency of such weather is lower. It highlights the unique challenges posed by adverse weather and the potential for more severe accidents in these conditions.\n\nOne limitation of this analysis is that it does not account for driving rates during different weather conditions. Without additional data, it's challenging to establish crash rates relative to the frequency of specific weather types. If more comprehensive data were available, it would be possible to calculate crash rates per mile driven or per hour of exposure to provide a more accurate representation of the risks associated with each weather condition.\n\n::: {#cell-Heatmap-for-weather-by-severity-and-age .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# summarizing data using a pivot table\npivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Weather Conditions', aggfunc='count')\n\n# Normalize pivot table\nnorm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 6))\nheatmap = sns.heatmap(norm_pivot, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar=True)\nplt.xticks(rotation=45)  # Rotate x-axis tick labels\nplt.yticks(rotation=45)  # Rotate y-axis tick labels\nplt.title('Heatmap of Weather Conditions by Age Group')\nplt.xlabel('Weather Conditions')\nplt.ylabel('Age Group')\ncbar = heatmap.collections[0].colorbar  # Get the colorbar\ncbar.set_label('Proportion of Crash Severity')  # Indicate proportion of crash types within a group\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/heatmap-for-weather-by-severity-and-age-output-1.png){#heatmap-for-weather-by-severity-and-age width=787 height=565}\n:::\n:::\n\n\nThe heatmap depicting the relationship between age groups and weather conditions provides insights into the frequency and severity of crashes under varying weather circumstances. Notably, the majority of non-fatal crashes occur in clear weather conditions. This observation aligns with the previous finding that clear conditions are associated with the highest overall crash counts.\n\n::: {#cell-Visualizations-for-crash-severity-and-road-surface-conditions .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Road Surface Condition', hue='Crash Severity', data=crash_data, palette='coolwarm')\nplt.title('Crash Severity by Road Surface Condition')\nplt.xlabel('Road Surface Condition')\nplt.ylabel('Number of Crashes')\nplt.legend(title='Crash Severity')\nplt.xticks(rotation=75) \nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualizations-for-crash-severity-and-road-surface-conditions-output-1.png){#visualizations-for-crash-severity-and-road-surface-conditions width=685 height=685}\n:::\n:::\n\n\nAn analysis of road surface conditions indicates that dry roads have the highest count of overall crashes. This is likely due to the prevalence of dry roads during typical driving conditions and higher traffic volumes. However, road surfaces like wet and snowy also account for a significant number of crashes, highlighting the importance of traction in crash prevention.\n\n::: {#cell-Heatmap-for-road-surface-condition-by-severity-and-age .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# summarizing data using a pivot table\npivot_table = pd.pivot_table(crash_data, values='Crash Severity', index='Age Group', columns='Road Surface Condition', aggfunc='count')\n\n# Normalize pivot table\nnorm_pivot = pivot_table.div(pivot_table.sum(axis=1), axis=0)\n\nheatmap = sns.heatmap(norm_pivot, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', cbar=True)\nplt.xticks(rotation=75)  # Rotate x-axis tick labels\nplt.yticks(rotation=45)  # Rotate y-axis tick labels\nplt.title('Heatmap of Road surfaces and Age Groups')\nplt.xlabel('Road Surface Condition')\nplt.ylabel('Age Group')\ncbar = heatmap.collections[0].colorbar  # Get the colorbar\ncbar.set_label('Proportion of Crash Severity')  # Indicate proportion of crash types within a group\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/heatmap-for-road-surface-condition-by-severity-and-age-output-1.png){#heatmap-for-road-surface-condition-by-severity-and-age width=594 height=611}\n:::\n:::\n\n\nThe heatmap displaying road surface conditions and age groups offers valuable insights into the safety implications of various road surfaces. A notable observation is that unknown and unreported surface conditions are associated with a significant proportion of severe crashes. This might indicate challenges in data collection and reporting by various agencies, suggesting that incomplete data could obscure important safety risks.\n\nDespite having fewer overall crashes, icy, snowy, and wet roads exhibit higher rates of severe crashes. This finding underscores the danger posed by reduced traction and adverse weather conditions. The correlation between these road surface conditions and crash severity supports the need for additional safety measures, such as improved road maintenance, better reporting practices, and driver education on navigating challenging road conditions.\n\nOur analysis has provided a clear understanding of the variables most closely associated with crash severity, shedding light on the factors that significantly impact crash outcomes. This knowledge serves as a solid foundation for the modeling process detailed in Question 2, where we hope to build predictive models that leverage these insights. The findings also highlight the pronounced imbalance between no-injury crashes and highly severe crashes, emphasizing the need for public agencies and Departments of Transportation (DOTs) to focus on safety measures for reducing severe incidents. By addressing these disparities and targeting the key variables related to crash severity, we can contribute to improved road safety and more effective traffic management strategies.\n\n### Question 2:\n\nThe initial analysis from question 1 yielded interesting insights into the relationship between age and crash severity, along with environmental factors like lighting, weather, and road conditions. These findings help identify which age groups are most at risk and the circumstances that contribute to severe crashes. Given these insights, we now move to question 2, where the goal is to create a predictive model to classify crash severity.\n\nTo start, we need to preprocess the crash data by filtering out rows where the severity is unknown. Next, we create a binary variable to distinguish crashes with \"no injury\" (property damage only) from those involving injuries or fatalities. This step is crucial due to the heavy imbalance of fatal crashes, which are relatively rare. This binary classification allows for a more straightforward modeling approach, focusing on predicting the likelihood of crashes resulting in injury or fatality. Below, we create a table to display the count of no-injury crashes and injury/fatality crashes to understand the distribution of our target variable.\n\n::: {#create-binary-feature-variable .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\n# Filter rows where the severity is unknow\ncrash_data = crash_data[crash_data['Crash Severity'] != \"Unknown\"]\n\n# Add a new column named 'feature_variable'\ncrash_data['feature_variable'] = [0 if x == 'No injury' else 1 for x in crash_data['Crash Severity']]\n\n# Drop the 'Crash Severity' column\ncrash_data = crash_data.drop('Crash Severity', axis=1)\n\n# Create a count table for the new feature variable\nseverity_counts = crash_data['feature_variable'].value_counts().rename({0: 'No Injury', 1: 'Injury/Fatality'})\n\n# Display the count table\nprint(severity_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo Injury          18996\nInjury/Fatality     5617\nName: feature_variable, dtype: int64\n```\n:::\n:::\n\n\nWith the target variable established, it is important to explore its relationships with a specific set of feature variables. These variables were chosen based on preliminary analysis and fundamental concepts in traffic engineering, recognizing that certain factors are closely associated with crash severity.\n\n- Speed Limit: Known to be correlated with crash severity.\n- Light Conditions: Affects visibility and safety.\n- Weather Conditions: Influences road conditions and crash likelihood.\n- Road Surface Condition: Determines traction and safety.\n- Roadway Junction Type: Indicates types of intersections and their risks.\n- Traffic Control Device Type: Affects traffic flow and safety.\n- Manner of Collision: Describes the nature of crash events.\n- Age: A demographic factor.\n- Sex: Another demographic factor.\n\nThe following plots include a correlation matrix and a pair plot. The correlation matrix shows that the numeric variables have little to no correlation with each other, indicating independence between them. The pair plot provides a more detailed visualization of the relationships among the numeric features, helping to identify potential patterns or trends not immediately apparent from the raw data.\n\n::: {#visualize-relationship-between-numeric-features-and-target-variable .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# Select certain feature variables based on analysis in Q1 and understanding of traffic engineering\ncolumns_to_keep = [\n    'feature_variable',\n    'Light Conditions',\n    'Manner of Collision',\n    'Road Surface Condition',\n    'Roadway Junction Type',\n    'Traffic Control Device Type',\n    'Weather Conditions',\n    'Speed Limit',\n    'Age',\n    'Sex'\n]\n\n# Create the subset from the crash_data DataFrame\nmodel_crash_data = crash_data[columns_to_keep]\n\n\n# Select only numerical columns to create a subset\nnumerical_crash_data = model_crash_data.select_dtypes(include=['int64', 'float64'])\n\n# Now create the correlation matrix with the subset\ncorrelation_matrix = numerical_crash_data.corr()\n\n# Create a heatmap for the correlation matrix\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n# Create a pairplot for the numerical subset\nsns.pairplot(numerical_crash_data)\n# plt.title(\"Pairplot for Numerical Columns\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-numeric-features-and-target-variable-output-1.png){#visualize-relationship-between-numeric-features-and-target-variable-1 width=533 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-numeric-features-and-target-variable-output-2.png){#visualize-relationship-between-numeric-features-and-target-variable-2 width=710 height=709}\n:::\n:::\n\n\nFollowing this, the report includes bar plots for each of the categorical columns and their relationships with the feature variables. These plots serve to highlight the distribution of the categorical data, offering a clearer understanding of how these features relate to the target variable. This analysis aims to uncover meaningful patterns that can guide further investigations and inform safety measures in traffic engineering.\n\n::: {#visualize-relationship-between-categorical-features-and-target-variable .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# Perform minor feature engineering for variables with excessive options\n\n# Create a mapping for the \"Sex\" column\nsex_mapping = {\n    \"F - Female\": \"F\",\n    \"M - Male\": \"M\",\n    \"U - Unknown\": \"U\",\n    \"X - Non-Binary\": \"X\"\n}\n\n# Apply the mapping to the \"Sex\" column\nmodel_crash_data[\"Sex\"] = model_crash_data[\"Sex\"].map(sex_mapping)\n\n# Define the age bins and labels\nage_bins = [0, 16, 17, 20, 24, 34, 44, 54, 64, 74, 84, 200]\nage_labels = [\"<16\", \"16-17\", \"18-20\", \"21-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-74\", \"75-84\", \">84\"]\n\n# Apply binning to the \"Age\" column\nmodel_crash_data[\"Age\"] = pd.cut(model_crash_data[\"Age\"], bins=age_bins, labels=age_labels, right=False)\n\n# Stacked bar plot for Sex and feature_variable\nsns.countplot(x='Sex', hue='feature_variable', data=model_crash_data)\nplt.title(\"Stacked Bar Plot for Sex and feature_variable\")\nplt.xticks(rotation=45)\nplt.show()\n\n# Stacked bar plot for Traffic Control Device Type and feature_variable\nsns.countplot(x='Traffic Control Device Type', hue='feature_variable', data=model_crash_data)\nplt.title(\"Stacked Bar Plot for Traffic Control Device Type and feature_variable\")\nplt.xticks(rotation=90)\nplt.show()\n\n# Stacked bar plot for Weather Conditions and feature_variable\nsns.countplot(x='Weather Conditions', hue='feature_variable', data=model_crash_data)\nplt.title(\"Stacked Bar Plot for Weather Conditions and feature_variable\")\nplt.xticks(rotation=45)\nplt.show()\n\n# Box plot for Age Group and feature_variable\nsns.countplot(x='Age', hue='feature_variable', data=model_crash_data)\nplt.title(\"Box Plot for Age Group and feature_variable\")\nplt.xticks(rotation=45)\nplt.show()\n\n# Crosstab for Roadway Junction Type and feature_variable\nsns.countplot(x='Roadway Junction Type', hue='feature_variable', data=model_crash_data)\nplt.title(\"Box Plot for Roadway Junction Type and feature_variable\")\nplt.xticks(rotation=75)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-categorical-features-and-target-variable-output-1.png){#visualize-relationship-between-categorical-features-and-target-variable-1 width=610 height=454}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-categorical-features-and-target-variable-output-2.png){#visualize-relationship-between-categorical-features-and-target-variable-2 width=623 height=665}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-categorical-features-and-target-variable-output-3.png){#visualize-relationship-between-categorical-features-and-target-variable-3 width=610 height=491}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-categorical-features-and-target-variable-output-4.png){#visualize-relationship-between-categorical-features-and-target-variable-4 width=602 height=473}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/visualize-relationship-between-categorical-features-and-target-variable-output-5.png){#visualize-relationship-between-categorical-features-and-target-variable-5 width=610 height=589}\n:::\n:::\n\n\nIn this section, we meticulously examine the dataset for missing values, distinguishing between numerical and categorical columns. Addressing missing data is crucial for ensuring the integrity and reliability of subsequent analyses. By systematically scrutinizing both numerical and categorical columns, we aim to identify any gaps in the dataset and determine the appropriate course of action. This meticulous approach allows us to maintain the quality of the data and make informed decisions regarding data imputation or removal.\n\n::: {#data-cleaning-and-missing-value-analysis .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\n# Find numerical columns\nnumerical_cols = model_crash_data.select_dtypes(include = ['int64', 'float64'])\n\n# Calculate missing values count for each numerical column\nmissing_values_count = numerical_cols.isnull().sum()\n\n# Calculate missing rate for each numerical column\nmissing_rate = (missing_values_count / len(model_crash_data)) * 100\n\nmissing_data = pd.DataFrame({\n    'Missing Values': missing_values_count,\n    'Percentage (%)': missing_rate\n})\n\nprint('Analysis of Missing Values for numerical features: \\n\\n', missing_data, '\\n\\n')\n\n# Drop categorical columns with missing rate over 50%\ncolumns_to_drop = missing_rate[missing_rate > 50].index\nmodel_crash_data = model_crash_data.drop(columns_to_drop, axis=1)\n\n# Find categorical columns\ncategorical_columns = model_crash_data.select_dtypes(include = ['object', 'category'])\n\n# Calculate missing values count for each categorical column\nmissing_values_count = categorical_columns.isnull().sum()\n\n# Calculate missing rate for each categorical column\nmissing_rate = (missing_values_count / len(crash_data)) * 100\n\nmissing_data = pd.DataFrame({\n    'Missing Values': missing_values_count,\n    'Percentage (%)': missing_rate\n})\n\nprint('Analysis of Missing Values for categorical features: \\n\\n', missing_data, '\\n\\n')\n\n# Drop categorical columns with missing rate over 50%\ncolumns_to_drop = missing_rate[missing_rate > 50].index\ncrash_data = crash_data.drop(columns_to_drop, axis=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Missing Values for numerical features: \n\n                   Missing Values  Percentage (%)\nfeature_variable               0        0.000000\nSpeed Limit                 1984        8.060781 \n\n\nAnalysis of Missing Values for categorical features: \n\n                              Missing Values  Percentage (%)\nLight Conditions                          0        0.000000\nManner of Collision                       3        0.012189\nRoad Surface Condition                    0        0.000000\nRoadway Junction Type                     3        0.012189\nTraffic Control Device Type               3        0.012189\nWeather Conditions                        0        0.000000\nAge                                       0        0.000000\nSex                                    1556        6.321862 \n\n\n```\n:::\n:::\n\n\n\n\nGiven the critical nature of this analysis, handling missing values is a significant concern. The decision was made to remove rows with missing data rather than impute. This choice was driven by the observation that the column with the highest number of missing values had only 8% of its entries missing. By removing these rows, we avoid introducing bias that could arise from imputation, which is a particularly sensitive issue in crash modeling.\n\nRegarding data standardization and encoding, the \"Speed Limit\" variable was converted to a categorical data type. This decision reflects the fact that speed limits are often discrete and do not behave like continuous numerical variables. Treating them as categorical eliminates the risk of implying linear relationships or gradients where they do not exist.\n\nFor other categorical features, such as intersection type and weather conditions, one-hot encoding was employed. This approach was chosen over label encoding because it avoids the implication of ordinality among categorical variables. Label encoding could suggest an inherent order or ranking between categories, which is not appropriate for these types of features.\n\nBy using one-hot encoding, we retain the categorical nature of these features while preparing them for use in machine learning models. This step ensures that the encoded data accurately reflects the characteristics of the original dataset without introducing unintended biases.\n\n::: {#cell-Encoding-categorical-variables .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\n# Convert \"Speed Limit\" to a categorical data type\nmodel_crash_data_cleaned['Speed Limit'] = model_crash_data_cleaned['Speed Limit'].astype('category')\n\n# Select categorical columns\ncategorical_columns = model_crash_data_cleaned.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Categorical Columns:\")\nprint(categorical_columns)\nprint()\n\n# One-hot encode categorical variables\ncrash_data_encoded = pd.get_dummies(model_crash_data_cleaned, columns=categorical_columns, drop_first=True)\n\nprint(\"One-Hot Encoded Data:\")\ncrash_data_encoded.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCategorical Columns:\n['Light Conditions', 'Manner of Collision', 'Road Surface Condition', 'Roadway Junction Type', 'Traffic Control Device Type', 'Weather Conditions', 'Speed Limit', 'Age', 'Sex']\n\nOne-Hot Encoded Data:\n```\n:::\n\n::: {#encoding-categorical-variables .cell-output .cell-output-display execution_count=132}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_variable</th>\n      <th>Light Conditions_Dark - no lighting</th>\n      <th>Light Conditions_Dark - unknown lighting</th>\n      <th>Light Conditions_Dawn</th>\n      <th>Light Conditions_Daylight</th>\n      <th>Light Conditions_Dusk</th>\n      <th>Light Conditions_Not reported</th>\n      <th>Light Conditions_Other</th>\n      <th>Light Conditions_Unknown</th>\n      <th>Manner of Collision_Front to Front</th>\n      <th>...</th>\n      <th>Age_25-34</th>\n      <th>Age_35-44</th>\n      <th>Age_45-54</th>\n      <th>Age_55-64</th>\n      <th>Age_65-74</th>\n      <th>Age_75-84</th>\n      <th>Age_&gt;84</th>\n      <th>Sex_M</th>\n      <th>Sex_U</th>\n      <th>Sex_X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 88 columns</p>\n</div>\n```\n:::\n:::\n\n\n\n\n::: {#train-test-split .cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X_train: (17071, 87)\nShape of X_test: (4268, 87)\nShape of y_train: (17071,)\nShape of y_test: (4268,)\n```\n:::\n:::\n\n\n\n\nFollowing the data preprocessing and encoding steps, the next phase involves defining and evaluating four distinct models: logistic regression, decision tree, random forest, and K-nearest neighbors (KNN). These models represent a range of approaches to classification, from linear methods to ensemble techniques and distance-based algorithms.\n\nTo assess the performance of these models, the dataset was split into training and testing sets using an 80/20 ratio, with 80% of the data used for training and 20% for testing. This split allows for robust evaluation of the models' ability to generalize to new data.\n\nBelow, we report the results for each model using key metrics: accuracy, precision, recall, and F1 score. These metrics offer a comprehensive view of model performance, highlighting not only the overall accuracy but also the ability to correctly identify positive and negative cases (precision), the rate of true positive predictions (recall), and the balance between precision and recall (F1 score).\n\n::: {#model-validation .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n# List of classifiers\nclassifiers = [log_reg, dtree, rf_classifier, knn]\n\n# Perform cross-validation and compute evaluation metrics for each classifier\nfor classifier in classifiers:\n    # Cross-validation\n    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n\n    # Compute evaluation metrics\n    accuracy = cv_scores.mean()\n    precision = precision_score(y_test, classifier.predict(X_test))\n    recall = recall_score(y_test, classifier.predict(X_test))\n    f1 = f1_score(y_test, classifier.predict(X_test))\n    \n    # Print the results\n    print('Classifier: ', str(classifier))\n    print('Accuracy: ', accuracy)\n    print('Precision: ', precision)\n    print('Recall: ', recall)\n    print('F1-Score: ', f1)\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClassifier:  LogisticRegression(random_state=9)\nAccuracy:  0.7709567786077652\nPrecision:  0.43478260869565216\nRecall:  0.01936108422071636\nF1-Score:  0.03707136237256719\n\nClassifier:  DecisionTreeClassifier()\nAccuracy:  0.7529139594864314\nPrecision:  0.5145413870246085\nRecall:  0.4453049370764763\nF1-Score:  0.47742605085625317\n\nClassifier:  RandomForestClassifier()\nAccuracy:  0.782379591056034\nPrecision:  0.5930232558139535\nRecall:  0.345595353339787\nF1-Score:  0.4366972477064221\n\nClassifier:  KNeighborsClassifier()\nAccuracy:  0.7628725916281336\nPrecision:  0.5079646017699115\nRecall:  0.27783155856727976\nF1-Score:  0.3591989987484356\n\n```\n:::\n:::\n\n\n\n\nTo evaluate the performance of our classifiers, we plotted the Receiver Operating Characteristic (ROC) curve and calculated the Area Under the Curve (AUC). The ROC curve helps us understand the trade-off between the True Positive Rate and the False Positive Rate, providing a visual representation of the model's ability to distinguish between classes. A higher AUC value indicates a better-performing model, with a perfect classifier achieving an AUC of 1.\n\nIn the following plot, you will see ROC curves for K-Nearest Neighbors, Decision Tree, Random Forest, and Logistic Regression classifiers. Among these models, the Random Forest classifier had the highest AUC, indicating that it was the closest to the top-left corner of the ROC plot, demonstrating strong discriminative ability. This makes Random Forest the most promising model among those tested.\n\n::: {#cell-Plotting-ROC-AUC .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\n# Plot ROC curves for different classifiers\nplt.figure(figsize=(8, 6))  # Set the plot size\n\n# ROC curve for KNN with AUC\nplt.plot(fpr_knn, tpr_knn, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_knn:.2f}) for KNN')\n\n# ROC curve for Decision Tree\nplt.plot(fpr_tree, tpr_tree, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_tree:.2f}) for Decision Tree')\n\n# ROC curve for Random Forest\nplt.plot(fpr_forest, tpr_forest, color='red', lw=2, label=f'ROC curve (AUC = {roc_auc_forest:.2f}) for Random Forest')\n\n# ROC curve for Logistic Regression\nplt.plot(fpr_log, tpr_log, color='green', lw=2, label=f'ROC curve (AUC = {roc_auc_log:.2f}) for Logistic Regression')\n\n# Diagonal line representing random guessing\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier\n\n# Set plot limits and labels\nplt.xlim([0, 1])  # X-axis from 0 to 1 (False Positive Rate)\nplt.ylim([0, 1.05])  # Y-axis from 0 to slightly above 1 (True Positive Rate)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')  # Title\n\n# Display the legend in the lower right corner\nplt.legend(loc='lower right')\n\n# Show the plot\nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plotting-roc-auc-output-1.png){#plotting-roc-auc width=674 height=523}\n:::\n:::\n\n\n\n\nTo further examine model performance, we turn to confusion matrices, which provide a detailed breakdown of predictions versus actual outcomes. These matrices are particularly useful for identifying issues with class imbalance and evaluating model tendencies.\n\nThe confusion matrices presented below reveal a key insight: the models tend to predict 0 (non-severe crashes) far more frequently than 1 (severe crashes). This tendency is a common consequence of imbalanced data, where the majority class overwhelms the minority class. While this approach can yield high accuracy, it often comes at the expense of poor recall and precision, especially for the minority class.\n\nThese findings align with the earlier observation that our models, despite high accuracy, often fall short in terms of precision, recall, and F1 score. By examining these confusion matrices, we can better understand how model predictions are skewed and what adjustments might be needed to improve overall performance.\n\n::: {#cell-Confusion-matrices .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\n# Create a 2x2 grid for the subplots\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))  # Define the grid structure\n\n# Confusion Matrix for Logistic Regression\ncm = confusion_matrix(y_test, predictions_log)\nsns.heatmap(cm, annot=True, fmt='g', ax=axs[0, 0])  # Plot in the top-left\naxs[0, 0].set_title('Logistic Regression Confusion Matrix',fontdict={\"size\":10})  # Set the title\n\n# Confusion Matrix for KNN\ncm = confusion_matrix(y_test, predictions_knn)\nsns.heatmap(cm, annot=True, fmt='g', ax=axs[0, 1])  # Plot in the top-right\naxs[0, 1].set_title('KNN Confusion Matrix',fontdict={\"size\":10})\n\n# Confusion Matrix for Decision Tree\ncm = confusion_matrix(y_test, predictions_tree)\nsns.heatmap(cm, annot=True, fmt='g', ax=axs[1, 0])  # Plot in the bottom-left\naxs[1, 0].set_title('Decision Tree Confusion Matrix',fontdict={\"size\":10})\n\n# Confusion Matrix for Random Forest\ncm = confusion_matrix(y_test, predictions_forest)\nsns.heatmap(cm, annot=True, fmt='g', ax=axs[1, 1])  # Plot in the bottom-right\naxs[1, 1].set_title('Random Forest Confusion Matrix',fontdict={\"size\":10})\n\n# Set common x and y labels\nfor ax in axs.flat:\n    ax.set_ylabel('Actual label')\n    ax.set_xlabel('Predicted label')\n\n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot with all subplots\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/confusion-matrices-output-1.png){#confusion-matrices width=751 height=563}\n:::\n:::\n\n\n## Discussion of Results & Conclusions\n\nThe objective of this project was to analyze the relationship between various features and a target variable to understand crash severity and evaluate the performance of different classifiers. After establishing a set of key feature variables, including 'Speed Limit', 'Light Conditions', 'Weather Conditions', 'Road Surface Condition', 'Roadway Junction Type', 'Traffic Control Device Type', 'Manner of Collision', 'Age', and 'Sex', we proceeded to build and test four machine learning models: Logistic Regression, Decision Tree, Random Forest, and K-Nearest Neighbors (KNN).\n\nWhile all models achieved an accuracy of approximately 78%, it became evident that accuracy alone wasn't a sufficient measure due to the imbalanced nature of the dataset. This led us to examine additional metrics such as precision, recall, and F1 score, which offer more insights into model performance in the context of class imbalance. These metrics reveal that models tended to predict the majority class (non-severe crashes), yielding high accuracy but low recall and precision for the minority class (severe crashes).\n\nAmong the four classifiers, the Random Forest (RF) model demonstrated the best performance. It achieved a higher true positive rate, leading to improved recall, precision, and F1 score compared to other models. This result suggests that RF's ensemble nature and ability to handle diverse data make it particularly effective for this type of analysis.\n\nDespite the promising results with Random Forest, there are several areas for future research and improvement. For instance, additional metrics, such as processing time and resource utilization, could be considered to evaluate model efficiency. Furthermore, addressing class imbalance through resampling techniques or class weights could enhance model accuracy and reliability for the minority class. Exploring different feature engineering approaches, integrating more contextual data, or experimenting with other machine learning algorithms may also yield improved outcomes.\n\nIn conclusion, this study highlights the challenges associated with imbalanced data and underscores the importance of considering multiple performance metrics beyond accuracy. Random Forest proved to be a strong candidate for predicting crash severity, but further research and refinement are needed to build more robust and efficient models. Future studies could focus on enhancing recall and precision for minority classes and exploring additional features that contribute to crash dynamics.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}